{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-IL-_bVr5Zp"
      },
      "source": [
        "#TP04 - QA Bot basado\n",
        "\n",
        "El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxC9mY7bsZ-T"
      },
      "source": [
        "## 1 - Cargamos de los datos y las librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3lyCfzLtBvs",
        "outputId": "171cb2c4-435a-40e0-ed2f-54555b228c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install --upgrade --no-cache-dir gdown --quiet\n",
        "!pip install fasttext\n",
        "!pip install --upgrade tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "import string\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Activation, Dropout, Dense, Flatten, LSTM, SimpleRNN\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from keras.layers import Input\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este dataset aborda el desafío de responder automáticamente a preguntas generadas a partir de texto de artículos de Wikipedia. Incluye tres archivos de preguntas (S08, S09, S10) con preguntas, respuestas, y niveles de dificultad. El texto de Wikipedia usado contiene 690,000 palabras. El dataset fue recopilado por investigadores de Carnegie Mellon y la Universidad de Pittsburgh entre 2008 y 2010.\n",
        "\n",
        "![images.jfif](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "Nj9Wx6R8uoiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URLs de los archivos en Google Drive\n",
        "urls = [\n",
        "    'https://drive.google.com/uc?id=1ibDp-rfVGaUdBXfw6Rbkvr2H8FQBi-tW',\n",
        "    'https://drive.google.com/uc?id=1x7WRCBXY-ZhDZI4D9EKyZytNpOJwS2nt',\n",
        "    'https://drive.google.com/uc?id=1FZUtXLRMTvb8p2U2VneLFyzHI1xWI37f'\n",
        "]\n",
        "\n",
        "# Descargar los archivos\n",
        "for url in urls:\n",
        "    gdown.download(url, quiet=False)\n",
        "\n",
        "# Cargar los archivos en DataFrames\n",
        "d1 = pd.read_csv('S08_question_answer_pairs.txt', sep='\\t')\n",
        "d2 = pd.read_csv('S09_question_answer_pairs.txt', sep='\\t')\n",
        "d3 = pd.read_csv('S10_question_answer_pairs.txt', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Concatenar los DataFrames\n",
        "dat2 = pd.concat([d1, d2, d3], ignore_index=True)\n",
        "\n",
        "# Asegurarse de que todos los valores sean cadenas\n",
        "dat2['Question'] = dat2['Question'].astype(str)\n",
        "dat2['Answer'] = dat2['Answer'].astype(str)\n",
        "\n",
        "# Crear la columna 'Question' combinando 'ArticleTitle' y 'Question'\n",
        "dat2['Question'] = dat2['ArticleTitle'].str.replace('_', ' ') + ' ' + dat2['Question']\n",
        "\n",
        "# Seleccionar las columnas necesarias y eliminar duplicados\n",
        "dat2 = dat2[['Question', 'Answer']]\n",
        "dat2 = dat2.drop_duplicates(subset='Question')\n",
        "\n",
        "# Convertir dat2 a una estructura con entradas independientes para preguntas y respuestas\n",
        "def convert_to_format(dat2):\n",
        "    converted_data = []\n",
        "    for i in range(len(dat2)):\n",
        "        question_text = str(dat2.iloc[i]['Question'])\n",
        "        answer_text = str(dat2.iloc[i]['Answer'])\n",
        "\n",
        "        # Validar que los textos sean cadenas antes de añadirlos\n",
        "        if not isinstance(question_text, str):\n",
        "            raise TypeError(\"Question text must be a string.\")\n",
        "        if not isinstance(answer_text, str):\n",
        "            raise TypeError(\"Answer text must be a string.\")\n",
        "\n",
        "        # Crear las entradas para pregunta y respuesta\n",
        "        entry_question = {\n",
        "            'text': question_text,\n",
        "            'type': 'Question'\n",
        "        }\n",
        "        entry_answer = {\n",
        "            'text': answer_text,\n",
        "            'type': 'Answer'\n",
        "        }\n",
        "\n",
        "        # Añadir los dos mensajes por separado\n",
        "        converted_data.append(entry_question)\n",
        "        converted_data.append(entry_answer)\n",
        "\n",
        "    return converted_data\n",
        "\n",
        "# Aplicar la conversión\n",
        "data3 = convert_to_format(dat2)\n",
        "\n",
        "# Verificar el resultado\n",
        "print(data3[:2])  # Mostrar las primeras dos entradas para verificar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KddT9TIRU3Ql",
        "outputId": "faa7ad60-d889-49d6-e3bb-b7afcd428e6b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ibDp-rfVGaUdBXfw6Rbkvr2H8FQBi-tW\n",
            "To: /content/S08_question_answer_pairs.txt\n",
            "100%|██████████| 181k/181k [00:00<00:00, 67.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1x7WRCBXY-ZhDZI4D9EKyZytNpOJwS2nt\n",
            "To: /content/S09_question_answer_pairs.txt\n",
            "100%|██████████| 85.4k/85.4k [00:00<00:00, 52.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FZUtXLRMTvb8p2U2VneLFyzHI1xWI37f\n",
            "To: /content/S10_question_answer_pairs.txt\n",
            "100%|██████████| 166k/166k [00:00<00:00, 84.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': 'Abraham Lincoln Was Abraham Lincoln the sixteenth President of the United States?', 'type': 'Question'}, {'text': 'yes', 'type': 'Answer'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar el resultado\n",
        "print(data3[:1])  # Mostrar las primeras dos entradas para verificar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLfPQPlxcNZF",
        "outputId": "a4b13b20-f25e-4177-f226-da818395ea87"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': 'Abraham Lincoln Was Abraham Lincoln the sixteenth President of the United States?', 'type': 'Question'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2EDWdpCwG58",
        "outputId": "20251cb6-b1d9-4624-f1a8-0d8d581050dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows utilizadas: 2416\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Configuración\n",
        "max_len = 30\n",
        "\n",
        "# Función para limpiar el texto\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = txt.replace(\"'d\", \" had\")\n",
        "    txt = txt.replace(\"'s\", \" is\")\n",
        "    txt = txt.replace(\"'m\", \" am\")\n",
        "    txt = txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "    return txt\n",
        "\n",
        "# Inicializar listas para almacenar las oraciones\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "\n",
        "# Convertir `dat2` a una lista de diccionarios con preguntas y respuestas\n",
        "dat2_list = dat2.to_dict(orient='records')\n",
        "\n",
        "for line in dat2_list:\n",
        "    chat_in = clean_text(line['Question'])\n",
        "    chat_out = clean_text(line['Answer'])\n",
        "\n",
        "    if len(chat_in.split()) >= max_len or len(chat_out.split()) >= max_len:\n",
        "        continue\n",
        "\n",
        "    # Añadir la oración de entrada y salida\n",
        "    input_sentences.append(chat_in)\n",
        "    output_sentences.append(chat_out + ' ')\n",
        "    output_sentences_inputs.append(' ' + chat_out)\n",
        "\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjxxZgVHwLIc",
        "outputId": "d53fb9bf-ba58-485b-e352-29e224137be9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('abraham lincoln did lincoln sign the national banking act of 1863 ',\n",
              " 'yes ',\n",
              " ' yes')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ugV-3f0wgrm"
      },
      "source": [
        "##2 - Preprocesamiento.\n",
        "\n",
        "word2idx_inputs, max_input_len\n",
        "word2idx_outputs, max_out_len, num_words_output\n",
        "encoder_input_sequences, decoder_output_sequences, decoder_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "364gkTf3wY2r",
        "outputId": "27e3444f-188c-4f26-dac8-3df278da0f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_input_len: 29\n",
            "max_out_len: 29\n",
            "word2idx_inputs: 4269 palabras\n",
            "word2idx_outputs: 2784 palabras\n"
          ]
        }
      ],
      "source": [
        "# Tokenización de las secuencias de entrada\n",
        "tokenizer_inputs = Tokenizer()\n",
        "tokenizer_inputs.fit_on_texts(input_sentences)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_sentences)\n",
        "\n",
        "# Longitud máxima de la secuencia de entrada\n",
        "max_input_len = max(len(seq) for seq in input_sequences)\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "\n",
        "# Tokenización de las secuencias de salida\n",
        "tokenizer_outputs = Tokenizer()\n",
        "tokenizer_outputs.fit_on_texts(output_sentences + output_sentences_inputs)\n",
        "output_sequences = tokenizer_outputs.texts_to_sequences(output_sentences)\n",
        "output_sequences_inputs = tokenizer_outputs.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "# Longitud máxima de la secuencia de salida\n",
        "max_out_len = max(len(seq) for seq in output_sequences)\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# Padding de las secuencias\n",
        "encoder_input_sequences = pad_sequences(input_sequences, maxlen=max_input_len)\n",
        "decoder_output_sequences = pad_sequences(output_sequences, maxlen=max_out_len)\n",
        "decoder_targets = pad_sequences(output_sequences_inputs, maxlen=max_out_len)\n",
        "\n",
        "# Resultados\n",
        "print(f\"max_input_len: {max_input_len}\")\n",
        "print(f\"max_out_len: {max_out_len}\")\n",
        "print(f\"word2idx_inputs: {len(word2idx_inputs)} palabras\")\n",
        "print(f\"word2idx_outputs: {num_words_output} palabras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyMdJaN32mHi"
      },
      "source": [
        "##3 - Preparar los embeddings\n",
        "\n",
        "Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn8Vn25OxCW8",
        "outputId": "5cbeaef5-8dda-431a-a887-a86710aa1bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-29 04:51:41--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-08-29 04:51:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-08-29 04:51:41--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.83MB/s    in 2m 39s  \n",
            "\n",
            "2024-08-29 04:54:21 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "pW3R4p6Aw0LT"
      },
      "outputs": [],
      "source": [
        "# Descargar los embeddings de Glove\n",
        "embedding_dim = 100\n",
        "embeddings_index = {}\n",
        "\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = vector\n",
        "\n",
        "# Preparar la matriz de embeddings\n",
        "num_words = len(word2idx_inputs) + 1\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in word2idx_inputs.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Capa de Embedding\n",
        "embedding_layer = Embedding(\n",
        "    num_words,\n",
        "    embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    #input_length=max_input_len,\n",
        "    trainable=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abXfcH4z3QGJ"
      },
      "source": [
        "Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drh15VNg3Q1J",
        "outputId": "13dba3aa-5416-49b8-fddb-c599c4992e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 172ms/step - accuracy: 0.8149 - loss: 5.6879 - val_accuracy: 0.8352 - val_loss: 1.7594\n",
            "Epoch 2/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 257ms/step - accuracy: 0.8929 - loss: 1.1122 - val_accuracy: 0.8352 - val_loss: 1.2292\n",
            "Epoch 3/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 134ms/step - accuracy: 0.8939 - loss: 0.7588 - val_accuracy: 0.8352 - val_loss: 1.1328\n",
            "Epoch 4/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 252ms/step - accuracy: 0.8949 - loss: 0.6929 - val_accuracy: 0.8468 - val_loss: 1.0947\n",
            "Epoch 5/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - accuracy: 0.9096 - loss: 0.6332 - val_accuracy: 0.8549 - val_loss: 1.0739\n",
            "Epoch 6/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 293ms/step - accuracy: 0.9054 - loss: 0.6675 - val_accuracy: 0.8549 - val_loss: 1.0580\n",
            "Epoch 7/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 147ms/step - accuracy: 0.9116 - loss: 0.6090 - val_accuracy: 0.8549 - val_loss: 1.0433\n",
            "Epoch 8/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 186ms/step - accuracy: 0.9103 - loss: 0.6092 - val_accuracy: 0.8549 - val_loss: 1.0284\n",
            "Epoch 9/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 157ms/step - accuracy: 0.9107 - loss: 0.5966 - val_accuracy: 0.8549 - val_loss: 1.0164\n",
            "Epoch 10/10\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 200ms/step - accuracy: 0.9087 - loss: 0.5993 - val_accuracy: 0.8549 - val_loss: 1.0073\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a2b84926350>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "# Definir el modelo\n",
        "latent_dim = 64\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_input_len,))\n",
        "x = embedding_layer(encoder_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_out_len,))\n",
        "decoder_embedding_layer = Embedding(num_words_output, latent_dim)\n",
        "decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Definir el modelo completo\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Reshape de los targets para la entrada en el modelo\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "    (len(input_sentences), max_out_len, num_words_output),\n",
        "    dtype='float32'\n",
        ")\n",
        "\n",
        "for i, d in enumerate(decoder_targets):\n",
        "    for t, word in enumerate(d):\n",
        "        decoder_targets_one_hot[i, t, word] = 1\n",
        "\n",
        "# Entrenamiento\n",
        "model.fit(\n",
        "    [encoder_input_sequences, decoder_output_sequences],\n",
        "    decoder_targets_one_hot,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_split=0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GIfJvIz3wQC"
      },
      "source": [
        "##4 - Entrenar al modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSXlVJR43zEh",
        "outputId": "1b7deb4b-1335-4301-a0ec-5fb82eb13592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 203ms/step - accuracy: 0.8513 - loss: 2.6920 - val_accuracy: 0.8352 - val_loss: 1.1415\n",
            "Epoch 2/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 199ms/step - accuracy: 0.9042 - loss: 0.6582 - val_accuracy: 0.8547 - val_loss: 1.0758\n",
            "Epoch 3/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 199ms/step - accuracy: 0.9136 - loss: 0.6047 - val_accuracy: 0.8549 - val_loss: 1.0552\n",
            "Epoch 4/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 154ms/step - accuracy: 0.9166 - loss: 0.5674 - val_accuracy: 0.8564 - val_loss: 1.0311\n",
            "Epoch 5/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 206ms/step - accuracy: 0.9076 - loss: 0.6191 - val_accuracy: 0.8579 - val_loss: 1.0042\n",
            "Epoch 6/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 183ms/step - accuracy: 0.9153 - loss: 0.5719 - val_accuracy: 0.8611 - val_loss: 0.9877\n",
            "Epoch 7/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 205ms/step - accuracy: 0.9166 - loss: 0.5765 - val_accuracy: 0.8642 - val_loss: 0.9785\n",
            "Epoch 8/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 160ms/step - accuracy: 0.9203 - loss: 0.5490 - val_accuracy: 0.8681 - val_loss: 0.9695\n",
            "Epoch 9/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 217ms/step - accuracy: 0.9225 - loss: 0.5384 - val_accuracy: 0.8681 - val_loss: 0.9635\n",
            "Epoch 10/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 184ms/step - accuracy: 0.9249 - loss: 0.5225 - val_accuracy: 0.8697 - val_loss: 0.9577\n",
            "Epoch 11/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 196ms/step - accuracy: 0.9243 - loss: 0.5241 - val_accuracy: 0.8702 - val_loss: 0.9522\n",
            "Epoch 12/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 154ms/step - accuracy: 0.9267 - loss: 0.5018 - val_accuracy: 0.8730 - val_loss: 0.9490\n",
            "Epoch 13/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 200ms/step - accuracy: 0.9297 - loss: 0.4855 - val_accuracy: 0.8738 - val_loss: 0.9469\n",
            "Epoch 14/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 149ms/step - accuracy: 0.9293 - loss: 0.4919 - val_accuracy: 0.8752 - val_loss: 0.9419\n",
            "Epoch 15/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 196ms/step - accuracy: 0.9278 - loss: 0.5038 - val_accuracy: 0.8770 - val_loss: 0.9376\n",
            "Epoch 16/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 151ms/step - accuracy: 0.9259 - loss: 0.5169 - val_accuracy: 0.8778 - val_loss: 0.9335\n",
            "Epoch 17/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 200ms/step - accuracy: 0.9270 - loss: 0.5086 - val_accuracy: 0.8797 - val_loss: 0.9298\n",
            "Epoch 18/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 174ms/step - accuracy: 0.9302 - loss: 0.4893 - val_accuracy: 0.8817 - val_loss: 0.9263\n",
            "Epoch 19/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 229ms/step - accuracy: 0.9255 - loss: 0.5224 - val_accuracy: 0.8832 - val_loss: 0.9224\n",
            "Epoch 20/20\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 201ms/step - accuracy: 0.9264 - loss: 0.5139 - val_accuracy: 0.8827 - val_loss: 0.9192\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a2b49b9f310>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "# Definir el modelo\n",
        "latent_dim = 136\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_input_len,))\n",
        "x = embedding_layer(encoder_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_out_len,))\n",
        "decoder_embedding_layer = Embedding(num_words_output, latent_dim)\n",
        "decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Definir el modelo completo\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Reshape de los targets para la entrada en el modelo\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "    (len(input_sentences), max_out_len, num_words_output),\n",
        "    dtype='float32'\n",
        ")\n",
        "\n",
        "for i, d in enumerate(decoder_targets):\n",
        "    for t, word in enumerate(d):\n",
        "        decoder_targets_one_hot[i, t, word] = 1\n",
        "\n",
        "# Entrenamiento\n",
        "model.fit(\n",
        "    [encoder_input_sequences, decoder_output_sequences],\n",
        "    decoder_targets_one_hot,\n",
        "    batch_size=8,\n",
        "    epochs=15,\n",
        "    validation_split=0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rfPjrQ04T8x"
      },
      "source": [
        "##5 - Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcXVpeJ54XLR",
        "outputId": "af33410b-7724-4092-8125-c51d45c76c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a2b84db7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
            "Error: 'comienzo' no se encuentra en el diccionario.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a2b84db79a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Input: abraham lincoln was abraham lincoln the sixteenth president of the united states \n",
            "Output:  yes the nan of and was to was\n"
          ]
        }
      ],
      "source": [
        "# Modelo encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Modelo decoder\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs_x, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Función de predicción\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Check if 'start' exists in word2idx_outputs, if not, handle accordingly\n",
        "    if 'start' in word2idx_outputs:\n",
        "        target_seq[0, 0] = word2idx_outputs['start']\n",
        "    else:\n",
        "        # Use 'comienzo' as the SOS token\n",
        "        if 'comienzo' in word2idx_outputs:\n",
        "            target_seq[0, 0] = word2idx_outputs['comienzo']\n",
        "        else:\n",
        "            # Handle the case where 'comienzo' is not in word2idx_outputs\n",
        "            print(\"Error: 'comienzo' no se encuentra en el diccionario.\")\n",
        "            # Puedes retornar un valor por defecto o manejar el error de otra manera\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        # Adjust index to account for starting at 1\n",
        "        sampled_token_index = sampled_token_index + 1\n",
        "        sampled_word = tokenizer_outputs.index_word[sampled_token_index]\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_sentence) > max_out_len:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Prueba del modelo\n",
        "test_sentence = encoder_input_sequences[0:1]\n",
        "translated_sentence = decode_sequence(test_sentence)\n",
        "print(f\"Input: {input_sentences[0]}\")\n",
        "print(f\"Output: {translated_sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeUKg12Bd9tP"
      },
      "source": [
        "#Bot QA 🤖\n",
        "Construir QA Bot basado en el ejemplo del traductor pero con un dataset QA.\n",
        "\n",
        "* MAX_VOCAB_SIZE = 8000\n",
        "* max_length ~ 10\n",
        "* Embeddings 300 Fasttext\n",
        "* n_units = 128\n",
        "* LSTM Dropout 0.2\n",
        "* Epochs 30~50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preparación de los Embeddings con FastText"
      ],
      "metadata": {
        "id": "uYyi-9btd94G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_KU9_70xtLP",
        "outputId": "b86dcf82-6c4a-45dc-cc37-2e54ea91820d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Descargar y cargar el modelo de FastText\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # modelo en inglés\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "# Parámetros\n",
        "MAX_VOCAB_SIZE = 8000\n",
        "embedding_dim = 300\n",
        "\n",
        "# Tokenización de las secuencias de entrada\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer_inputs.fit_on_texts(input_sentences)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_sentences)\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "max_input_len = min(max(len(seq) for seq in input_sequences), 10)\n",
        "\n",
        "# Limitar el vocabulario a MAX_VOCAB_SIZE\n",
        "num_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs) + 1)\n",
        "\n",
        "# Crear la matriz de embeddings\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i < MAX_VOCAB_SIZE:\n",
        "        embedding_vector = ft.get_word_vector(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Capa de Embedding\n",
        "embedding_layer = Embedding(\n",
        "    num_words,\n",
        "    embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    input_length=max_input_len,\n",
        "    trainable=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuración del Modelo Encoder-Decoder"
      ],
      "metadata": {
        "id": "RGPD6j5hd2SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "latent_dim = 128  # Tamaño de las unidades LSTM\n",
        "max_out_len = 10  # Longitud máxima de las secuencias de salida\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "x = embedding_layer(encoder_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True, dropout=0.2)\n",
        "encoder_outputs, state_h, state_c = encoder(x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding_layer = Embedding(num_words, latent_dim)\n",
        "decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Modelo completo\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "QST8t_WEcorW"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preparación de los Datos y Entrenamiento del Modelo"
      ],
      "metadata": {
        "id": "CddeFMexeff1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "icceBxcWZZg-"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "source": [
        "# Convertir secuencias de salida en one-hot encoding\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer_outputs.fit_on_texts(output_sentences)\n",
        "output_sequences = tokenizer_outputs.texts_to_sequences(output_sentences)\n",
        "output_sequences_inputs = tokenizer_outputs.texts_to_sequences(output_sentences_inputs)\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "num_words_output = len(word2idx_outputs) + 1  # El +1 es necesario para incluir el índice 0\n",
        "\n",
        "# Padding de secuencias ANTES de generar one-hot encoding\n",
        "# Esto asegura que todas las secuencias tengan la misma longitud\n",
        "encoder_input_sequences = pad_sequences(input_sequences, maxlen=max_input_len)\n",
        "decoder_output_sequences = pad_sequences(output_sequences, maxlen=max_out_len, padding='post') # Asegurar que el padding sea 'post'\n",
        "decoder_targets = pad_sequences(output_sequences_inputs, maxlen=max_out_len, padding='post') # Asegurar que el padding sea 'post'\n",
        "\n",
        "# Convertir secuencias de salida en one-hot encoding DESPUÉS del padding\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "    (len(input_sequences), max_out_len, num_words_output),\n",
        "    dtype='float32'\n",
        ")\n",
        "\n",
        "for i, d in enumerate(decoder_targets): # Usar 'decoder_targets' que ya tienen padding\n",
        "    for t, word in enumerate(d):\n",
        "        if word < num_words_output: # Make sure word index is within bounds\n",
        "            decoder_targets_one_hot[i, t, word] = 1\n",
        "\n",
        "# Ensure the model is compiled only once, before training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model.fit(\n",
        "    [encoder_input_sequences, decoder_output_sequences],\n",
        "    decoder_targets_one_hot,\n",
        "    batch_size=64,\n",
        "    epochs=30,  # Ajustable entre 30-50 epochs\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5r0G9LxYt0u",
        "outputId": "e321b029-c52e-4ea6-8542-03808efd08ee"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.6329 - loss: 5.9609 - val_accuracy: 0.6093 - val_loss: 3.1586\n",
            "Epoch 2/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 706ms/step - accuracy: 0.7366 - loss: 2.0223 - val_accuracy: 0.6093 - val_loss: 2.7638\n",
            "Epoch 3/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 746ms/step - accuracy: 0.7394 - loss: 1.7131 - val_accuracy: 0.6260 - val_loss: 2.6561\n",
            "Epoch 4/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 764ms/step - accuracy: 0.7498 - loss: 1.7077 - val_accuracy: 0.6413 - val_loss: 2.6048\n",
            "Epoch 5/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 853ms/step - accuracy: 0.7730 - loss: 1.5529 - val_accuracy: 0.6537 - val_loss: 2.5513\n",
            "Epoch 6/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 460ms/step - accuracy: 0.7788 - loss: 1.5083 - val_accuracy: 0.6564 - val_loss: 2.5237\n",
            "Epoch 7/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 373ms/step - accuracy: 0.7642 - loss: 1.5956 - val_accuracy: 0.6562 - val_loss: 2.5035\n",
            "Epoch 8/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 374ms/step - accuracy: 0.7625 - loss: 1.5784 - val_accuracy: 0.6570 - val_loss: 2.4807\n",
            "Epoch 9/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 351ms/step - accuracy: 0.7705 - loss: 1.4968 - val_accuracy: 0.6570 - val_loss: 2.4550\n",
            "Epoch 10/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 361ms/step - accuracy: 0.7764 - loss: 1.4645 - val_accuracy: 0.6632 - val_loss: 2.4342\n",
            "Epoch 11/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 355ms/step - accuracy: 0.7913 - loss: 1.4053 - val_accuracy: 0.6632 - val_loss: 2.4156\n",
            "Epoch 12/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 377ms/step - accuracy: 0.7886 - loss: 1.3961 - val_accuracy: 0.6700 - val_loss: 2.3996\n",
            "Epoch 13/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 359ms/step - accuracy: 0.7913 - loss: 1.3899 - val_accuracy: 0.6638 - val_loss: 2.3889\n",
            "Epoch 14/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 398ms/step - accuracy: 0.7818 - loss: 1.4373 - val_accuracy: 0.6756 - val_loss: 2.3767\n",
            "Epoch 15/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 344ms/step - accuracy: 0.7760 - loss: 1.4782 - val_accuracy: 0.6798 - val_loss: 2.3663\n",
            "Epoch 16/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 378ms/step - accuracy: 0.7904 - loss: 1.3850 - val_accuracy: 0.6711 - val_loss: 2.3568\n",
            "Epoch 17/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 355ms/step - accuracy: 0.7950 - loss: 1.3350 - val_accuracy: 0.6843 - val_loss: 2.3507\n",
            "Epoch 18/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 363ms/step - accuracy: 0.7924 - loss: 1.3861 - val_accuracy: 0.6853 - val_loss: 2.3439\n",
            "Epoch 19/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 369ms/step - accuracy: 0.8067 - loss: 1.3092 - val_accuracy: 0.6870 - val_loss: 2.3378\n",
            "Epoch 20/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 352ms/step - accuracy: 0.7914 - loss: 1.3986 - val_accuracy: 0.6890 - val_loss: 2.3351\n",
            "Epoch 21/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 350ms/step - accuracy: 0.8040 - loss: 1.3232 - val_accuracy: 0.6886 - val_loss: 2.3285\n",
            "Epoch 22/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 370ms/step - accuracy: 0.8013 - loss: 1.3305 - val_accuracy: 0.6969 - val_loss: 2.3224\n",
            "Epoch 23/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 353ms/step - accuracy: 0.8000 - loss: 1.3561 - val_accuracy: 0.6928 - val_loss: 2.3217\n",
            "Epoch 24/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 353ms/step - accuracy: 0.8089 - loss: 1.2750 - val_accuracy: 0.6957 - val_loss: 2.3161\n",
            "Epoch 25/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 408ms/step - accuracy: 0.8045 - loss: 1.3062 - val_accuracy: 0.6957 - val_loss: 2.3122\n",
            "Epoch 26/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 369ms/step - accuracy: 0.8209 - loss: 1.1981 - val_accuracy: 0.6961 - val_loss: 2.3097\n",
            "Epoch 27/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 348ms/step - accuracy: 0.8103 - loss: 1.2763 - val_accuracy: 0.6965 - val_loss: 2.3060\n",
            "Epoch 28/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 348ms/step - accuracy: 0.8095 - loss: 1.2753 - val_accuracy: 0.7033 - val_loss: 2.3029\n",
            "Epoch 29/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 349ms/step - accuracy: 0.7997 - loss: 1.3393 - val_accuracy: 0.7012 - val_loss: 2.2985\n",
            "Epoch 30/30\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 349ms/step - accuracy: 0.8017 - loss: 1.3143 - val_accuracy: 0.7025 - val_loss: 2.2959\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a2b8c16ac20>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "source": [
        "# Modelo encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Modelo decoder\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs_x, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Obtener los estados del encoder\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Inicializar la secuencia de entrada para el decoder con el token de inicio\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Use the first value in word2idx_outputs if 'start' or 'comienzo' are not found\n",
        "    start_token = word2idx_outputs.get('start', word2idx_outputs.get('comienzo', next(iter(word2idx_outputs.values()))))\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        # Obtener predicciones del modelo del decoder\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Obtener el índice de la palabra con la mayor probabilidad\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer_outputs.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        # Si no se encuentra una palabra, romper el bucle\n",
        "        if sampled_word == '':\n",
        "            print(\"Advertencia: índice de palabra desconocido. Interrumpiendo la decodificación.\")\n",
        "            break\n",
        "\n",
        "        # Añadir la palabra decodificada a la oración\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Condiciones de parada\n",
        "        if (sampled_word == 'end') or (len(decoded_sentence.split()) >= max_out_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Actualizar la secuencia de entrada para el próximo paso de decodificación\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Actualizar los estados internos del LSTM\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "    # Pruebas con preguntas específicas\n",
        "questions = [\n",
        "    \"Do you read?\",\n",
        "    \"Do you have any pet?\",\n",
        "    \"Where are you from?\"\n",
        "]\n",
        "\n",
        "# Initialize and fit tokenizer_inputs on the input sentences\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "# Assuming 'input_sentences' is defined somewhere with your input data\n",
        "tokenizer_inputs.fit_on_texts(input_sentences)\n",
        "\n",
        "for question in questions:\n",
        "    input_sequence = tokenizer_inputs.texts_to_sequences([question])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_input_len)\n",
        "    translated_sentence = decode_sequence(input_sequence)\n",
        "    print(f\"Input: {question}\")\n",
        "    print(f\"Output: {translated_sentence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBZCjc5p4h4S",
        "outputId": "350181bb-8754-46c1-8c56-eeaaace85ee4"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 823ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Advertencia: índice de palabra desconocido. Interrumpiendo la decodificación.\n",
            "Input: Do you read?\n",
            "Output: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Advertencia: índice de palabra desconocido. Interrumpiendo la decodificación.\n",
            "Input: Do you have any pet?\n",
            "Output: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Advertencia: índice de palabra desconocido. Interrumpiendo la decodificación.\n",
            "Input: Where are you from?\n",
            "Output: yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Input, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "latent_dim = 256\n",
        "embedding_dim = 100\n",
        "MAX_VOCAB_SIZE = 8000\n",
        "\n",
        "# Crear el modelo encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n"
      ],
      "metadata": {
        "id": "dFmpD0qi8JRf"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Crear el modelo decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(MAX_VOCAB_SIZE, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + encoder_states, [decoder_outputs] + [state_h, state_c])\n"
      ],
      "metadata": {
        "id": "m8Tzm-XP8zuJ"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Obtener los estados del encoder\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Inicializar la secuencia de entrada para el decoder con el token de inicio\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Usa el primer valor en word2idx_outputs si 'start' o 'comienzo' no se encuentran\n",
        "    start_token = word2idx_outputs.get('start', word2idx_outputs.get('comienzo', 1))  # Cambia 1 por el índice del token más probable si es necesario\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        # Obtener predicciones del modelo del decoder\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Obtener el índice de la palabra con la mayor probabilidad\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer_outputs.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        # Debugging: Imprimir el índice y la palabra muestreada\n",
        "        print(f\"Sampled Token Index: {sampled_token_index}, Sampled Word: {sampled_word}\")\n",
        "\n",
        "        # Si no se encuentra una palabra, romper el bucle\n",
        "        if sampled_word == '':\n",
        "            print(\"Advertencia: índice de palabra desconocido. Interrumpiendo la decodificación.\")\n",
        "            break\n",
        "\n",
        "        # Añadir la palabra decodificada a la oración\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Condiciones de parada\n",
        "        if (sampled_word == 'end') or (len(decoded_sentence.split()) >= max_out_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Actualizar la secuencia del decoder\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Actualizar los estados\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ],
      "metadata": {
        "id": "0U0OndTL-ebJ"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(dat2[:3])  # Muestra los primeros 3 elementos de la lista\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALmJNZg-_GZJ",
        "outputId": "af08ac5a-17f1-48ce-a89b-8c5f25723bcf"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Question Answer\n",
            "0  Abraham Lincoln Was Abraham Lincoln the sixtee...    yes\n",
            "2  Abraham Lincoln Did Lincoln sign the National ...    yes\n",
            "4   Abraham Lincoln Did his mother die of pneumonia?     no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los primeros 3 valores de la primera columna de `dat2`\n",
        "questions = dat2.iloc[:3, 0].tolist()\n",
        "\n",
        "# Pruebas con preguntas específicas\n",
        "for question in questions:\n",
        "    input_sequence = tokenizer_inputs.texts_to_sequences([question])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_input_len, padding='post')\n",
        "    translated_sentence = decode_sequence(input_sequence)\n",
        "    print(f\"Input: {question}\")\n",
        "    print(f\"Output: {translated_sentence}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGAK_VNq9edg",
        "outputId": "8346fb5b-4893-4c40-e8c5-26dbc37a9c5c"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Sampled Token Index: 434, Sampled Word: octopuses\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Sampled Token Index: 6421, Sampled Word: \n",
            "Advertencia: índice de palabra desconocido. Interrumpiendo la decodificación.\n",
            "Input: Abraham Lincoln Was Abraham Lincoln the sixteenth President of the United States?\n",
            "Output: octopuses\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Sampled Token Index: 12, Sampled Word: are\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Sampled Token Index: 322, Sampled Word: s\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Sampled Token Index: 513, Sampled Word: europe\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Sampled Token Index: 4917, Sampled Word: \n",
            "Advertencia: índice de palabra desconocido. Interrumpiendo la decodificación.\n",
            "Input: Abraham Lincoln Did Lincoln sign the National Banking Act of 1863?\n",
            "Output: are s europe\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Sampled Token Index: 374, Sampled Word: adult\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Sampled Token Index: 374, Sampled Word: adult\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Sampled Token Index: 2613, Sampled Word: diphthongs\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Sampled Token Index: 992, Sampled Word: released\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Sampled Token Index: 6574, Sampled Word: \n",
            "Advertencia: índice de palabra desconocido. Interrumpiendo la decodificación.\n",
            "Input: Abraham Lincoln Did his mother die of pneumonia?\n",
            "Output: adult adult diphthongs released\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://www.kaggle.com/datasets/rtatman/questionanswer-dataset?resource=download"
      ],
      "metadata": {
        "id": "Md3hifg1Vds8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRwo-qg636uJ"
      },
      "source": [
        "https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/blob/main/clase_6/ejercicios/ejercicios.md"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}