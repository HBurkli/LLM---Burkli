{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorización de texto y modelo de clasificación Naïve Bayes"
      ],
      "metadata": {
        "id": "Oa8fYZSvirvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups"
      ],
      "metadata": {
        "id": "OhETaeoKiRqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cIOEmb7oipmx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga de datos"
      ],
      "metadata": {
        "id": "2Nn9qaIpi7CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Cargar los datos de entrenamiento y prueba\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Verificar la carga de datos\n",
        "print(f'Número de documentos en el conjunto de entrenamiento: {len(newsgroups_train.data)}')\n",
        "print(f'Número de documentos en el conjunto de prueba: {len(newsgroups_test.data)}')\n",
        "\n",
        "# Mostrar un ejemplo de los datos\n",
        "print(\"\\nEjemplo de documento del conjunto de entrenamiento:\")\n",
        "print(newsgroups_train.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1PrCf6NnrJv",
        "outputId": "c8986026-e451-4ae2-cbc3-8c00247d5bb6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de documentos en el conjunto de entrenamiento: 11314\n",
            "Número de documentos en el conjunto de prueba: 7532\n",
            "\n",
            "Ejemplo de documento del conjunto de entrenamiento:\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes parámetros de instanciación en la documentación de sklearn\n",
        "tfidfvect = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "yQT2C_WBppbF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en el atributo `data` accedemos al texto\n",
        "newsgroups_train.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "oBJh0uhSckvx",
        "outputId": "bbdd414b-4098-4d47-c5f1-d7bd9c2d07c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador (obtener el vocabulario y calcular el vector IDF) y transformar directamente los datos\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-término"
      ],
      "metadata": {
        "id": "-QPKCp11cp-I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
      ],
      "metadata": {
        "id": "k0J7VD7vdDMQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en `y_train` guardamos los targets que son enteros\n",
        "y_train = newsgroups_train.target\n",
        "y_train[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZgR3gNjdoM_",
        "outputId": "a16b0908-1cf8-4f6c-f170-c767cf7ed5af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hay 20 clases correspondientes a los 20 grupos de noticias\n",
        "print(f'clases {np.unique(newsgroups_test.target)}')\n",
        "newsgroups_test.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-faMHbVreL6g",
        "outputId": "06d8279a-f234-4771-a7df-77ebda01d944"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clases [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos similaridad de documentos. Tomemos algún documento\n",
        "idx = 4811\n",
        "print(newsgroups_train.data[idx])"
      ],
      "metadata": {
        "id": "j0Tv36L-tnOE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6adaee2-0085-4049-e35a-4a6503f231a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE WHITE HOUSE\n",
            "\n",
            "                  Office of the Press Secretary\n",
            "                   (Pittsburgh, Pennslyvania)\n",
            "______________________________________________________________\n",
            "For Immediate Release                         April 17, 1993     \n",
            "\n",
            "             \n",
            "                  RADIO ADDRESS TO THE NATION \n",
            "                        BY THE PRESIDENT\n",
            "             \n",
            "                Pittsburgh International Airport\n",
            "                    Pittsburgh, Pennsylvania\n",
            "             \n",
            "             \n",
            "10:06 A.M. EDT\n",
            "             \n",
            "             \n",
            "             THE PRESIDENT:  Good morning.  My voice is coming to\n",
            "you this morning through the facilities of the oldest radio\n",
            "station in America, KDKA in Pittsburgh.  I'm visiting the city to\n",
            "meet personally with citizens here to discuss my plans for jobs,\n",
            "health care and the economy.  But I wanted first to do my weekly\n",
            "broadcast with the American people. \n",
            "             \n",
            "             I'm told this station first broadcast in 1920 when\n",
            "it reported that year's presidential elections.  Over the past\n",
            "seven decades presidents have found ways to keep in touch with\n",
            "the people, from whistle-stop tours to fire-side chats to the bus\n",
            "tour that I adopted, along with Vice President Gore, in last\n",
            "year's campaign.\n",
            "             \n",
            "             Every Saturday morning I take this time to talk with\n",
            "you, my fellow Americans, about the problems on your minds and\n",
            "what I'm doing to try and solve them.  It's my way of reporting\n",
            "to you and of giving you a way to hold me accountable.\n",
            "             \n",
            "             You sent me to Washington to get our government and\n",
            "economy moving after years of paralysis and policy and a bad\n",
            "experiment with trickle-down economics.  You know how important\n",
            "it is for us to make bold, comprehensive changes in the way we do\n",
            "business.  \n",
            "             \n",
            "             We live in a competitive global economy.  Nations\n",
            "rise and fall on the skills of their workers, the competitiveness\n",
            "of their companies, the imagination of their industries, and the\n",
            "cooperative experience and spirit that exists between business,\n",
            "labor and government.  Although many of the economies of the\n",
            "industrialized world are now suffering from slow growth, they've\n",
            "made many of the smart investments and the tough choices which\n",
            "our government has for too long ignored.  That's why many of them\n",
            "have been moving ahead and too many of our people have been\n",
            "falling behind.\n",
            "             \n",
            "             We have an economy today that even when it grows is\n",
            "not producing new jobs.  We've increased the debt of our nation\n",
            "by four times over the last 12 years, and we don't have much to\n",
            "show for it.  We know that wages of most working people have\n",
            "stopped rising, that most people are working longer work weeks\n",
            "and that too many families can no longer afford the escalating\n",
            "cost of health care.\n",
            "             \n",
            "             But we also know that, given the right tools, the\n",
            "right incentives and the right encouragement, our workers and\n",
            "businesses can make the kinds of products and profits our economy\n",
            "needs to expand opportunity and to make our communities better\n",
            "places to live.\n",
            "             \n",
            "             In many critical products today Americans are the\n",
            "low cost, high quality producers.  Our task is to make sure that\n",
            "we create more of those kinds of jobs.\n",
            "             \n",
            "             Just two months ago I gave Congress my plan for\n",
            "long-term jobs and economic growth.  It changes the old\n",
            "priorities in Washington and puts our emphasis where it needs to\n",
            "be -- on people's real needs, on increasing investments and jobs\n",
            "and education, on cutting the federal deficit, on stopping the\n",
            "waste which pays no dividends, and redirecting our precious\n",
            "resources toward investment that creates jobs now and lays the\n",
            "groundwork for robust economic growth in the future.\n",
            "             \n",
            "             These new directions passed the Congress in record\n",
            "time and created a new sense of hope and opportunity in our\n",
            "country.  Then the jobs plan I presented to Congress, which would\n",
            "create hundreds of thousands of jobs, most of them in the private\n",
            "sector in 1993 and 1994, passed the House of Representatives.  It\n",
            "now has the support of a majority of the United States Senate. \n",
            "But it's been held up by a filibuster of a minority in the\n",
            "Senate, just 43 senators.  They blocked a vote that they know\n",
            "would result in the passage of our bill and the creation of jobs.\n",
            "             \n",
            "             The issue isn't politics; the issue is people. \n",
            "Millions of Americans are waiting for this legislation and\n",
            "counting on it, counting on us in Washington.  But the jobs bill\n",
            "has been grounded by gridlock.  \n",
            "             \n",
            "             I know the American people are tired of business as\n",
            "usual and politics as usual.  I know they don't want us to spin\n",
            "or wheels.  They want the recovery to get moving.  So I have\n",
            "taken a first step to break this gridlock and gone the extra\n",
            "mile.  Yesterday I offered to cut the size of this plan by 25\n",
            "percent -- from $16 billion to $12 billion.  \n",
            "             \n",
            "             It's not what I'd hoped for.  With 16 million\n",
            "Americans looking for full-time work, I simply can't let the bill\n",
            "languish when I know that even a compromise bill will mean\n",
            "hundreds of thousands of jobs for our people.  The mandate is to\n",
            "act to achieve change and move the country forward.  By taking\n",
            "this initiative in the face of an unrelenting Senate talkathon, I\n",
            "think we can respond to your mandate and achieve a significant\n",
            "portion of our original goals.\n",
            "             \n",
            "             First, we want to keep the programs as much as\n",
            "possible that are needed to generate jobs and meet human needs,\n",
            "including highway and road construction, summer jobs for young\n",
            "people, immunization for children, construction of waste water\n",
            "sites, and aid to small businesses.  We also want to keep funding\n",
            "for extended unemployment compensation benefits, for people who\n",
            "have been unemployed for a long time because the economy isn't\n",
            "creating jobs.\n",
            "             \n",
            "             Second, I've recommended that all the other programs\n",
            "in the bill be cut across-the-board by a little more than 40\n",
            "percent.\n",
            "             \n",
            "             And third, I've recommended a new element in this\n",
            "program to help us immediately start our attempt to fight against\n",
            "crime by providing $200 million for cities and towns to rehire\n",
            "police officers who lost their jobs during the recession and put\n",
            "them back to work protecting our people.  I'm also going to fight\n",
            "for a tough crime bill because the people of this country need it\n",
            "and deserve it.\n",
            "             \n",
            "             Now, the people who are filibustering this bill --\n",
            "the Republican senators -- say they won't vote for it because it\n",
            "increases deficit spending, because there's extra spending this\n",
            "year that hasn't already been approved.  That sounds reasonable,\n",
            "doesn't it?  Here's what they don't say.  This program is more\n",
            "than paid for by budget cuts over my five-year budget, and this\n",
            "budget is well within the spending limits already approved by the\n",
            "Congress this year.\n",
            "             \n",
            "             It's amazing to me that many of these same senators\n",
            "who are filibustering the bill voted during the previous\n",
            "administration for billions of dollars of the same kind of\n",
            "emergency spending, and much of it was not designed to put the\n",
            "American people to work.  \n",
            "             \n",
            "             This is not about deficit spending.  We have offered\n",
            "a plan to cut the deficit.  This is about where your priorities\n",
            "are -- on people or on politics.  \n",
            "             \n",
            "             Keep in mind that our jobs bill is paid for dollar\n",
            "for dollar.  It is paid for by budget cuts.  And it's the\n",
            "soundest investment we can now make for ourselves and our\n",
            "children.  I urge all Americans to take another look at this jobs\n",
            "and investment program; to consider again the benefits for all of\n",
            "us when we've helped make more American partners working to\n",
            "ensure the future of our nation and the strength of our economy.\n",
            "             \n",
            "             You know, if every American who wanted a job had\n",
            "one, we wouldn't have a lot of the other problems we have in this\n",
            "country today.  This bill is not a miracle, it's a modest first\n",
            "step to try to set off a job creation explosion in this country\n",
            "again.  But it's a step we ought to take.  And it is fully paid\n",
            "for over the life of our budget.\n",
            "             \n",
            "             Tell your lawmakers what you think.  Tell them how\n",
            "important the bill is.  If it passes, we'll all be winners.\n",
            "             \n",
            "             Good morning, and thank you for listening.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# midamos la similaridad coseno con todos los documentos de train\n",
        "cossim = cosine_similarity(X_train[idx], X_train)[0]"
      ],
      "metadata": {
        "id": "shXJwzTvuTPS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "np.sort(cossim)[::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHvTh6bXwfkD",
        "outputId": "2b7afbb1-7067-4160-b617-1e78aeae2c5f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.70930477, 0.67474953, ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y a qué documentos corresponden\n",
        "np.argsort(cossim)[::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSWdiujowh3w",
        "outputId": "67d820eb-5685-40db-cca9-d0d112ee5e30"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4811,  6635,  4253, ...,  1534, 10055,  4750])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# los 5 documentos más similares:\n",
        "mostsim = np.argsort(cossim)[::-1][1:6]"
      ],
      "metadata": {
        "id": "Ub7O4lK5wk5w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# el documento original pertenece a la clase:\n",
        "newsgroups_train.target_names[y_train[idx]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "21X_4Am5wm6H",
        "outputId": "bfbf629e-ca75-4b25-be6f-5ffb3259c5e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y los 5 más similares son de las clases:\n",
        "for i in mostsim:\n",
        "  print(newsgroups_train.target_names[y_train[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA06p6-0wpYp",
        "outputId": "583e70f0-2f32-4b6d-bf11-ca5d8a89cbbf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de clasificación Naïve Bayes"
      ],
      "metadata": {
        "id": "0NG1Mng-wt-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "ULXnCNBowwy9",
        "outputId": "096302ef-e346-4084-d7fb-bb0b1e4a919d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "y_pred =  clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "pe2qs84Ew21l"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkGJhetEPdA4",
        "outputId": "62262e7d-60be-4e34-d984-c590ee9dc5bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_score(y_test, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desafío 1"
      ],
      "metadata": {
        "id": "HKiT7y5EnfRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vectorizar documentos.\n",
        "Tomar 5 documentos al azar y medir similaridad con el resto de los documentos. Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido la similaridad según el contenido del texto y la etiqueta de clasificación."
      ],
      "metadata": {
        "id": "YQdWoPC0nq1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar 5 índices aleatorios\n",
        "np.random.seed(42)  # Para reproducibilidad\n",
        "random_indices = np.random.choice(len(newsgroups_train.data), size=5, replace=False)\n",
        "\n",
        "# Obtener los textos y etiquetas correspondientes\n",
        "random_documents = [newsgroups_train.data[idx] for idx in random_indices]\n",
        "random_labels = [newsgroups_train.target[idx] for idx in random_indices]\n",
        "\n",
        "# Mostrar los textos seleccionados y sus etiquetas\n",
        "for i, text in enumerate(random_documents):\n",
        "    print(f\"Documento {i+1}, Etiqueta: {newsgroups_train.target_names[random_labels[i]]}\\n\")\n",
        "    print(text)\n",
        "    print(\"-------------------------------------------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8TbhT0Mm0Gj",
        "outputId": "48f42dba-0285-4183-db58-a70b82da84fa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento 1, Etiqueta: comp.sys.mac.hardware\n",
            "\n",
            "Could someone please post any info on these systems.\n",
            "\n",
            "Thanks.\n",
            "BoB\n",
            "-- \n",
            "---------------------------------------------------------------------- \n",
            "Robert Novitskey | \"Pursuing women is similar to banging one's head\n",
            "rrn@po.cwru.edu  |  against a wall...with less opportunity for reward\" \n",
            "-------------------------------------------------------------\n",
            "\n",
            "Documento 2, Etiqueta: comp.os.ms-windows.misc\n",
            "\n",
            "\n",
            "\n",
            "     Don't bother if you have CPBackup or Fastback.  They all offer options \n",
            "not available in the stripped-down MS version (FROM CPS!).  Examples - no \n",
            "proprietary format (to save space), probably no direct DMA access, and no \n",
            "tape drive!\n",
            "-------------------------------------------------------------\n",
            "\n",
            "Documento 3, Etiqueta: misc.forsale\n",
            "\n",
            "5.25\" Internal Low density disk drive.\n",
            "\n",
            "Monochrome monitor\n",
            "\n",
            "8088 motherboard, built in parallel and serial ports, built in mono and\n",
            "color output, 7Mhz.\n",
            "\n",
            "Libertarian, atheist, semi-anarchal Techno-Rat.\n",
            "-------------------------------------------------------------\n",
            "\n",
            "Documento 4, Etiqueta: talk.politics.guns\n",
            "\n",
            "Hi,\n",
            "\n",
            "In Canada, any gun that enters a National Park must be sealed (I think it's a\n",
            "small metal tag that's placed over the trigger).  The net result of this is\n",
            "that you _can't_ use a gun to protect yourself from bears (or psychos) in the\n",
            "National Parks.  Instead, one has to be sensitive to the dangers and annoyances\n",
            "of hiking in bear country, and take the appropriate precautions.\n",
            "\n",
            "I think this policy makes the users of the National Parks feel a little closer\n",
            "to Nature, that they are a part of Nature and, as such, have to deal with\n",
            "nature on it's own terms.\n",
            "-------------------------------------------------------------\n",
            "\n",
            "Documento 5, Etiqueta: rec.sport.hockey\n",
            "\n",
            "\n",
            "Doesn't it also have the Statue of Liberty on it or is that Richter's Mask?\n",
            "\n",
            "The back actually has a Bee followed by a Z to represent the Beezer. It \n",
            "also has something that looks like the three interconnecting circles from\n",
            "the Led Zepplin 4 album cover. Is that what it is supposed to be? and if\n",
            "it is does anybody know why he would put it there? Ali?\n",
            "\n",
            "\n",
            "John\n",
            "\"The official Language of Golf is Profanity\"\n",
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcular similaridad con el resto de los documentos"
      ],
      "metadata": {
        "id": "sjohF69gn4XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = []\n",
        "for i, idx in enumerate(random_indices):\n",
        "    print(f\"Documento {i+1} (Etiqueta: {newsgroups_train.target_names[random_labels[i]]})\\n\")\n",
        "    print(newsgroups_train.data[idx])\n",
        "    print(\"\\nDocumentos más similares:\\n\")\n",
        "\n",
        "    # Calcular similaridad coseno con respecto a todos los documentos\n",
        "    cossim = cosine_similarity(X_train[idx], X_train).flatten()\n",
        "    # Ordenar los índices por similaridad descendente (excluyendo el propio documento)\n",
        "    sorted_indices = np.argsort(cossim)[::-1][1:6]\n",
        "\n",
        "    for j, sim_idx in enumerate(sorted_indices):\n",
        "        print(f\"Similitud: {cossim[sim_idx]}\")\n",
        "        print(f\"Etiqueta: {newsgroups_train.target_names[y_train[sim_idx]]}\")\n",
        "        #print(newsgroups_train.data[sim_idx])\n",
        "        #print(\"-------------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab6U3dfZK5th",
        "outputId": "4b0a7523-31ee-4184-9333-87bfa15f1ccb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento 1 (Etiqueta: comp.sys.mac.hardware)\n",
            "\n",
            "Could someone please post any info on these systems.\n",
            "\n",
            "Thanks.\n",
            "BoB\n",
            "-- \n",
            "---------------------------------------------------------------------- \n",
            "Robert Novitskey | \"Pursuing women is similar to banging one's head\n",
            "rrn@po.cwru.edu  |  against a wall...with less opportunity for reward\" \n",
            "\n",
            "Documentos más similares:\n",
            "\n",
            "Similitud: 0.6665262187597427\n",
            "Etiqueta: comp.sys.mac.hardware\n",
            "Similitud: 0.34759575958927097\n",
            "Etiqueta: comp.sys.ibm.pc.hardware\n",
            "Similitud: 0.1798616722409978\n",
            "Etiqueta: comp.sys.mac.hardware\n",
            "Similitud: 0.1546515862768033\n",
            "Etiqueta: misc.forsale\n",
            "Similitud: 0.14143450876745123\n",
            "Etiqueta: comp.sys.mac.hardware\n",
            "Documento 2 (Etiqueta: comp.os.ms-windows.misc)\n",
            "\n",
            "\n",
            "\n",
            "     Don't bother if you have CPBackup or Fastback.  They all offer options \n",
            "not available in the stripped-down MS version (FROM CPS!).  Examples - no \n",
            "proprietary format (to save space), probably no direct DMA access, and no \n",
            "tape drive!\n",
            "\n",
            "Documentos más similares:\n",
            "\n",
            "Similitud: 0.20404925780984884\n",
            "Etiqueta: comp.sys.ibm.pc.hardware\n",
            "Similitud: 0.19242983739078068\n",
            "Etiqueta: comp.sys.ibm.pc.hardware\n",
            "Similitud: 0.17241546795722562\n",
            "Etiqueta: comp.sys.ibm.pc.hardware\n",
            "Similitud: 0.17092172981977347\n",
            "Etiqueta: comp.sys.ibm.pc.hardware\n",
            "Similitud: 0.16161396193853683\n",
            "Etiqueta: comp.sys.ibm.pc.hardware\n",
            "Documento 3 (Etiqueta: misc.forsale)\n",
            "\n",
            "5.25\" Internal Low density disk drive.\n",
            "\n",
            "Monochrome monitor\n",
            "\n",
            "8088 motherboard, built in parallel and serial ports, built in mono and\n",
            "color output, 7Mhz.\n",
            "\n",
            "Libertarian, atheist, semi-anarchal Techno-Rat.\n",
            "\n",
            "Documentos más similares:\n",
            "\n",
            "Similitud: 0.46224366625285723\n",
            "Etiqueta: misc.forsale\n",
            "Similitud: 0.2998530622881244\n",
            "Etiqueta: misc.forsale\n",
            "Similitud: 0.27401734026912405\n",
            "Etiqueta: comp.graphics\n",
            "Similitud: 0.2075713228490793\n",
            "Etiqueta: misc.forsale\n",
            "Similitud: 0.1685206152844338\n",
            "Etiqueta: comp.graphics\n",
            "Documento 4 (Etiqueta: talk.politics.guns)\n",
            "\n",
            "Hi,\n",
            "\n",
            "In Canada, any gun that enters a National Park must be sealed (I think it's a\n",
            "small metal tag that's placed over the trigger).  The net result of this is\n",
            "that you _can't_ use a gun to protect yourself from bears (or psychos) in the\n",
            "National Parks.  Instead, one has to be sensitive to the dangers and annoyances\n",
            "of hiking in bear country, and take the appropriate precautions.\n",
            "\n",
            "I think this policy makes the users of the National Parks feel a little closer\n",
            "to Nature, that they are a part of Nature and, as such, have to deal with\n",
            "nature on it's own terms.\n",
            "\n",
            "Documentos más similares:\n",
            "\n",
            "Similitud: 0.23640976321237323\n",
            "Etiqueta: talk.politics.guns\n",
            "Similitud: 0.23625331247794518\n",
            "Etiqueta: sci.crypt\n",
            "Similitud: 0.2328428305026939\n",
            "Etiqueta: talk.politics.misc\n",
            "Similitud: 0.22954088272089399\n",
            "Etiqueta: talk.politics.guns\n",
            "Similitud: 0.2290508813499399\n",
            "Etiqueta: alt.atheism\n",
            "Documento 5 (Etiqueta: rec.sport.hockey)\n",
            "\n",
            "\n",
            "Doesn't it also have the Statue of Liberty on it or is that Richter's Mask?\n",
            "\n",
            "The back actually has a Bee followed by a Z to represent the Beezer. It \n",
            "also has something that looks like the three interconnecting circles from\n",
            "the Led Zepplin 4 album cover. Is that what it is supposed to be? and if\n",
            "it is does anybody know why he would put it there? Ali?\n",
            "\n",
            "\n",
            "John\n",
            "\"The official Language of Golf is Profanity\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Documentos más similares:\n",
            "\n",
            "Similitud: 0.25139291047607\n",
            "Etiqueta: alt.atheism\n",
            "Similitud: 0.24798042501900502\n",
            "Etiqueta: soc.religion.christian\n",
            "Similitud: 0.2409581185911583\n",
            "Etiqueta: alt.atheism\n",
            "Similitud: 0.2409457104418431\n",
            "Etiqueta: soc.religion.christian\n",
            "Similitud: 0.2329408898945423\n",
            "Etiqueta: sci.crypt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clasificación Naïve Bayes\n",
        "Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación (f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial y ComplementNB."
      ],
      "metadata": {
        "id": "fvxIhz3wRHe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(vectorizer_params, model_type, model_params):\n",
        "    # Vectorización\n",
        "    vectorizer = TfidfVectorizer(**vectorizer_params)\n",
        "    X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "    X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "    # Selección del modelo\n",
        "    if model_type == 'MultinomialNB':\n",
        "        model = MultinomialNB(**model_params)\n",
        "    elif model_type == 'ComplementNB':\n",
        "        model = ComplementNB(**model_params)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type. Choose 'MultinomialNB' or 'ComplementNB'.\")\n",
        "\n",
        "    # Entrenamiento y predicción\n",
        "    model.fit(X_train, newsgroups_train.target)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluación\n",
        "    f1 = f1_score(newsgroups_test.target, y_pred, average='macro')\n",
        "    return f1"
      ],
      "metadata": {
        "id": "-bnAFfA5RaLM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diferentes parámetros del vectorizador"
      ],
      "metadata": {
        "id": "QUsZFsLGbCR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros del vectorizador para probar\n",
        "vectorizer_params_list = [\n",
        "    {'max_df': 0.5, 'min_df': 2, 'ngram_range': (1, 1), 'stop_words': 'english'},\n",
        "    {'max_df': 0.7, 'min_df': 5, 'ngram_range': (1, 2), 'stop_words': 'english'},\n",
        "    {'max_df': 0.8, 'min_df': 3, 'ngram_range': (1, 1), 'stop_words': None},\n",
        "    {'max_df': 1.0, 'min_df': 1, 'ngram_range': (1, 3), 'stop_words': 'english'}\n",
        "]\n",
        "\n",
        "# Parámetros del modelo Naïve Bayes para probar\n",
        "model_params = [\n",
        "    {},\n",
        "    {'alpha': 0.5},\n",
        "    {'alpha': 1.0, 'fit_prior': False}\n",
        "]\n",
        "\n",
        "best_f1_score = 0\n",
        "best_params = None\n",
        "\n",
        "for vectorizer_params in vectorizer_params_list:\n",
        "    for params in model_params:\n",
        "        for model_type in ['MultinomialNB', 'ComplementNB']:\n",
        "            f1 = train_and_evaluate(vectorizer_params, model_type, params)\n",
        "            if f1 > best_f1_score:\n",
        "                best_f1_score = f1\n",
        "                best_params = (vectorizer_params, model_type, params)\n",
        "\n",
        "print(f\"Mejor f1-score: {best_f1_score}\")\n",
        "print(f\"Mejores parámetros: {best_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA4BHDGtSIoz",
        "outputId": "6fb854fb-8333-4c91-fc3f-fe98fea8c3c7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor f1-score: 0.7072715088603252\n",
            "Mejores parámetros: ({'max_df': 1.0, 'min_df': 1, 'ngram_range': (1, 3), 'stop_words': 'english'}, 'ComplementNB', {'alpha': 0.5})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matriz documento-término\n",
        "Transponer la matriz documento-término. De esa manera se obtiene una matriz término-documento que puede ser interpretada como una colección de vectorización de palabras."
      ],
      "metadata": {
        "id": "jyzfxgQEbzYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorización\n",
        "tfidfvect = TfidfVectorizer()\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Transponer la matriz\n",
        "X_train_transposed = X_train.transpose()\n",
        "\n",
        "print(f'Tipo de X_train: {type(X_train)}')\n",
        "print(f'Tipo de X_train_transposed: {type(X_train_transposed)}')\n",
        "print(f'Shape de X_train: {X_train.shape}')\n",
        "print(f'Shape de X_train_transposed: {X_train_transposed.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfcnRjCzdy5E",
        "outputId": "5ccd709e-31a0-4889-fc15-c8e2c7d630a0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tipo de X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
            "Tipo de X_train_transposed: <class 'scipy.sparse._csc.csc_matrix'>\n",
            "Shape de X_train: (11314, 101631)\n",
            "Shape de X_train_transposed: (101631, 11314)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el índice de una palabra en particular\n",
        "word = 'car'\n",
        "word_index = tfidfvect.vocabulary_.get(word, -1)\n",
        "\n",
        "if word_index != -1:\n",
        "    # Obtener la fila correspondiente a la palabra\n",
        "    word_vector = X_train_transposed.getrow(word_index).toarray().flatten()\n",
        "    print(f'Vector de la palabra \"{word}\":\\n', word_vector)\n",
        "\n",
        "    # Mostrar los documentos donde esta palabra tiene mayor importancia\n",
        "    top_docs_indices = word_vector.argsort()[::-1][:5]\n",
        "    for idx in top_docs_indices:\n",
        "        print(f'\\nDocumento {idx} (Importancia: {word_vector[idx]}):')\n",
        "        print(newsgroups_train.data[idx])\n",
        "else:\n",
        "    print(f'La palabra \"{word}\" no se encuentra en el vocabulario.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlKIOm4dbmpp",
        "outputId": "3aa84aa6-faec-4a1c-d8f9-cc3e59bdfc95"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector de la palabra \"car\":\n",
            " [0.40464699 0.         0.         ... 0.         0.         0.        ]\n",
            "\n",
            "Documento 8266 (Importancia: 0.5287256881150304):\n",
            "\n",
            "Definitely!\n",
            "\n",
            "Safety is an important criterium for me when buying a car. I won't buy a \n",
            "small car like a Civic or whatever.\n",
            "\n",
            "Great = Safety + Handling + Speed  -  for me\n",
            "\n",
            "Seems to me that you would be more \"dead\" in a small car than a large car \n",
            "after an accident.\n",
            "\n",
            "Documento 8013 (Importancia: 0.46008568969406555):\n",
            "\n",
            "If you don't already know it, you should call the bank/credit union/\n",
            "finance company that holds the loan on your present car and get the\n",
            "current payoff cost.\n",
            "\n",
            "If you are trading in your current car on the new car, subtract the\n",
            "payoff amount from the trade-in the dealer is giving you.  (If this\n",
            "turns out to be a negative number, you need to reconsider the deal.)\n",
            "Subtract this difference from the price of the new car.  This is the\n",
            "size of the loan you will need for the new car.\n",
            "\n",
            "The dealer will take care of paying off the loan on your old car out\n",
            "of the money you give them when you pick up your new car.\n",
            "\n",
            "At least that's how it worked for me 5 years ago in Ohio...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Documento 8540 (Importancia: 0.44656404990398263):\n",
            "\n",
            "Poor Matthew.   A million posters to call \"you car drivers\" and he\n",
            "chooses me, a non car owner.\n",
            "\n",
            "Documento 7993 (Importancia: 0.4425021341721087):\n",
            "I bought a car with a defunct engine, to use for parts\n",
            "for my old but still running version of the same car.\n",
            "\n",
            "The car I bought has good tires.\n",
            "\n",
            "Is there anything in particular that I should do to\n",
            "store the defunct car long-term?  I'd hate to have\n",
            "parts of it go bad.  Someone has told me it's bad\n",
            "for the tires to not move the car once-in-a-while.\n",
            "Is this true?   Do I need some props to take the\n",
            "weight of the tires?\n",
            "\n",
            "Best to reply by mail, I am getting spotty news delivery.\n",
            "\n",
            "Documento 0 (Importancia: 0.4046469916999256):\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transponer la matriz documento-término y analizar similaridad entre palabras\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Transponer X_train para tener palabras como filas\n",
        "X_train_transposed = X_train.transpose()\n",
        "\n",
        "def analyze_word_similarity(X, vocabulary, words):\n",
        "    for word in words:\n",
        "        word_index = vocabulary.get(word, -1)\n",
        "        if word_index != -1:\n",
        "            # Obtener el vector de la palabra\n",
        "            word_vector = X.getrow(word_index).toarray()\n",
        "\n",
        "            # Calcular la similitud del coseno entre el vector de la palabra y todos los otros vectores de palabras\n",
        "            similarities = cosine_similarity(word_vector, X).flatten()\n",
        "\n",
        "            # Ordenar las palabras por similitud descendente\n",
        "            top_words_indices = similarities.argsort()[::-1][1:6]  # Excluir la palabra misma (similitud 1)\n",
        "\n",
        "            print(f'\\nPalabra: \"{word}\"')\n",
        "            for idx in top_words_indices:\n",
        "                term = idx2word[idx]\n",
        "                print(f'Palabra similar: \"{term}\" (Similitud de coseno: {similarities[idx]:.4f})')\n",
        "        else:\n",
        "            print(f'La palabra \"{word}\" no se encuentra en el vocabulario.')\n",
        "\n",
        "# Obtener el diccionario índice-palabra\n",
        "idx2word = {v: k for k, v in tfidfvect.vocabulary_.items()}\n",
        "\n",
        "# Seleccionar 5 palabras para analizar\n",
        "words_to_analyze = ['car', 'computer', 'space', 'health', 'government']\n",
        "\n",
        "# Analizar similaridad de palabras\n",
        "analyze_word_similarity(X_train_transposed, tfidfvect.vocabulary_, words_to_analyze)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDYezyS6eFGb",
        "outputId": "54f540b4-47d5-420f-a59d-af5e213e37f9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabra: \"car\"\n",
            "Palabra similar: \"cars\" (Similitud de coseno: 0.1797)\n",
            "Palabra similar: \"criterium\" (Similitud de coseno: 0.1770)\n",
            "Palabra similar: \"civic\" (Similitud de coseno: 0.1748)\n",
            "Palabra similar: \"owner\" (Similitud de coseno: 0.1689)\n",
            "Palabra similar: \"dealer\" (Similitud de coseno: 0.1681)\n",
            "\n",
            "Palabra: \"computer\"\n",
            "Palabra similar: \"decwriter\" (Similitud de coseno: 0.1563)\n",
            "Palabra similar: \"harkens\" (Similitud de coseno: 0.1522)\n",
            "Palabra similar: \"deluged\" (Similitud de coseno: 0.1522)\n",
            "Palabra similar: \"shopper\" (Similitud de coseno: 0.1443)\n",
            "Palabra similar: \"the\" (Similitud de coseno: 0.1361)\n",
            "\n",
            "Palabra: \"space\"\n",
            "Palabra similar: \"nasa\" (Similitud de coseno: 0.3304)\n",
            "Palabra similar: \"seds\" (Similitud de coseno: 0.2966)\n",
            "Palabra similar: \"shuttle\" (Similitud de coseno: 0.2928)\n",
            "Palabra similar: \"enfant\" (Similitud de coseno: 0.2803)\n",
            "Palabra similar: \"seti\" (Similitud de coseno: 0.2465)\n",
            "\n",
            "Palabra: \"health\"\n",
            "Palabra similar: \"ohip\" (Similitud de coseno: 0.3304)\n",
            "Palabra similar: \"provincial\" (Similitud de coseno: 0.2998)\n",
            "Palabra similar: \"care\" (Similitud de coseno: 0.2824)\n",
            "Palabra similar: \"traditionalists\" (Similitud de coseno: 0.2799)\n",
            "Palabra similar: \"fiscally\" (Similitud de coseno: 0.2799)\n",
            "\n",
            "Palabra: \"government\"\n",
            "Palabra similar: \"the\" (Similitud de coseno: 0.2410)\n",
            "Palabra similar: \"to\" (Similitud de coseno: 0.2251)\n",
            "Palabra similar: \"of\" (Similitud de coseno: 0.2227)\n",
            "Palabra similar: \"libertarian\" (Similitud de coseno: 0.2199)\n",
            "Palabra similar: \"encryption\" (Similitud de coseno: 0.2174)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slack\n",
        "* https://join.slack.com/t/ceaiworkspace/shared_invite/zt-2l9un8yte-yWrXdu7msfCwr32VG6RIgQ\n",
        "\n",
        "Github\n",
        "* https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/blob/main/clase_1/ejercicios/Desafio_1.ipynb\n",
        "\n",
        "Drive\n",
        "* https://drive.google.com/drive/u/0/folders/1joS44xgaoCKapc44WmkjcZyjWJh4tQz0\n",
        "\n",
        "Colab TP1\n",
        "* https://colab.research.google.com/github/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/blob/main/clase_1/ejercicios/Desafio_1.ipynb\n"
      ],
      "metadata": {
        "id": "6Kx60a3cgUGp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9xObsVhlhTs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}